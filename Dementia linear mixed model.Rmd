---
title: "Dementia linear mixed model"
output: html_document
date: "2023-08-08"
---

```{r library}

library(nlme)
```


```{r check longitudinal outcome mmse}

data_plot<-data_long.1[order(data_long.1$followupyears), ]

mmse_plot<-data_plot%>%
   select("study","id","idpicc","followupyears","visityear","mmse")

mmse_plot<-mmse_plot%>%
  arrange(id,followupyears)


ggplot(data = mmse_plot, aes(x = followupyears, y = mmse,group=id))+
    geom_line()

#check the one with all zero MMSE in followup years

mmse_plot%>%
  filter(mmse==0)

#1666
#1677
#1300

mmse_plot%>%
  filter(idpicc==1666) # 0 0

mmse_plot%>%
  filter(idpicc==1677) # 19 18 10 0 0

mmse_plot%>%
  filter(idpicc==1300) # 29 28 26 25 16 13 0 0 0

ggplot(data = subset(mmse_plot,study=="CamPalGN"), aes(x = followupyears, y = mmse,group=idpicc))+
    geom_line()

ggplot(data = subset(mmse_plot,study=="ParkWest"), aes(x = followupyears, y = mmse,group=idpicc))+
    geom_line()

ggplot(data = subset(mmse_plot,study=="PICNICS"), aes(x = followupyears, y = mmse,group=idpicc))+
    geom_line()

ggplot(data = subset(mmse_plot,study=="NYPUM"), aes(x = followupyears, y = mmse,group=idpicc))+
    geom_line()

ggplot(data = subset(mmse_plot,study=="PINE"), aes(x = followupyears, y = mmse,group=idpicc))+
    geom_line()

```

```{r baseline model}

baseline.cox.1<-coxph(Surv(years,cens)~age10+sex+yearseducation+mdsupdrs3.10+hybl+hallucinationsindex+cognitiveindex+strata(study),data=data.baseline.1, x = TRUE,model = TRUE)

#summary(baseline.cox.1)
#basehaz(baseline.cox.1)

baseline.cox.2<-coxph(Surv(years,cens)~age10+sex+yearseducation+mdsupdrs3.10+hybl+hallucinationsindex+cognitiveindex+strata(study),data=data.baseline.2, x = TRUE,model = TRUE)

baseline.cox.3<-coxph(Surv(years,cens)~age10+sex+yearseducation+mdsupdrs3.10+hybl+hallucinationsindex+cognitiveindex+strata(study),data=data.baseline.3, x = TRUE,model = TRUE)

baseline.cox.4<-coxph(Surv(years,cens)~age10+sex+yearseducation+mdsupdrs3.10+hybl+hallucinationsindex+cognitiveindex+strata(study),data=data.baseline.4, x = TRUE,model = TRUE)

baseline.cox.5<-coxph(Surv(years,cens)~age10+sex+yearseducation+mdsupdrs3.10+hybl+hallucinationsindex+cognitiveindex+strata(study),data=data.baseline.5, x = TRUE,model = TRUE)

baseline.cox.6<-coxph(Surv(years,cens)~age10+sex+yearseducation+mdsupdrs3.10+hybl+hallucinationsindex+cognitiveindex+strata(study),data=data.baseline.6, x = TRUE,model = TRUE)

baseline.cox.7<-coxph(Surv(years,cens)~age10+sex+yearseducation+mdsupdrs3.10+hybl+hallucinationsindex+cognitiveindex+strata(study),data=data.baseline.7, x = TRUE,model = TRUE)

baseline.cox.8<-coxph(Surv(years,cens)~age10+sex+yearseducation+mdsupdrs3.10+hybl+hallucinationsindex+cognitiveindex+strata(study),data=data.baseline.8, x = TRUE,model = TRUE)

baseline.cox.9<-coxph(Surv(years,cens)~age10+sex+yearseducation+mdsupdrs3.10+hybl+hallucinationsindex+cognitiveindex+strata(study),data=data.baseline.9, x = TRUE,model = TRUE)
```


```{r chose linear mixed model}

#----test to see which one works------

#As years of education only missing 11, so the best model select from first imputation data set can view as general

lme.1<-lme(mmse~ agebl+yearseducation, random = ~ followupyears | idpicc, data=data_long.1) 

lme.2<-lme(mmse~ sex+agebl+yearseducation, random = ~ followupyears | idpicc, data=data_long.1)

lme.3<-lme(mmse~ study+agebl+yearseducation, random = ~ followupyears | idpicc, data=data_long.1)

lme.4<-lme(mmse~ study*agebl+yearseducation, random = ~ followupyears | idpicc, data=data_long.1)

lme.5<-lme(mmse~ agebl+study*yearseducation, random = ~ followupyears | idpicc, data=data_long.1)

AIC(lme.1) #19800.31
AIC(lme.2) #19794.74
AIC(lme.3) #19786.14
AIC(lme.4) #19802.95
AIC(lme.5) #19794.21

BIC(lme.1) #19844.08
BIC(lme.2) #19844.76
BIC(lme.3) #19854.9
BIC(lme.4) #19896.7
BIC(lme.5) #19887.97

#Choose lme.1


lme.1.1<-lme(mmse~ agebl+yearseducation, random = ~ followupyears | idpicc, data=data_long.1) 

lme.1.2<-lme(mmse~ agebl+yearseducation, random = ~ followupyears | idpicc, data=data_long.2) 

lme.1.3<-lme(mmse~ agebl+yearseducation, random = ~ followupyears | idpicc, data=data_long.3) 

lme.1.4<-lme(mmse~ agebl+yearseducation, random = ~ followupyears | idpicc, data=data_long.4) 

lme.1.5<-lme(mmse~ agebl+yearseducation, random = ~ followupyears | idpicc, data=data_long.5) 

lme.1.6<-lme(mmse~ agebl+yearseducation, random = ~ followupyears | idpicc, data=data_long.6) 

lme.1.7<-lme(mmse~ agebl+yearseducation, random = ~ followupyears | idpicc, data=data_long.7) 

lme.1.8<-lme(mmse~ agebl+yearseducation, random = ~ followupyears | idpicc, data=data_long.8) 

lme.1.9<-lme(mmse~ agebl+yearseducation, random = ~ followupyears | idpicc, data=data_long.9) 

```

```{r joint model}

jointFit.1<-jm(baseline.cox.1, lme.1.1, time_var = "followupyears") #it works

jointFit.2<-jm(baseline.cox.2, lme.1.2, time_var = "followupyears") 

jointFit.3<-jm(baseline.cox.3, lme.1.3, time_var = "followupyears") 

jointFit.4<-jm(baseline.cox.4, lme.1.4, time_var = "followupyears") 

jointFit.5<-jm(baseline.cox.5, lme.1.5, time_var = "followupyears") 

jointFit.6<-jm(baseline.cox.6, lme.1.6, time_var = "followupyears") 

jointFit.7<-jm(baseline.cox.7, lme.1.7, time_var = "followupyears")

jointFit.8<-jm(baseline.cox.8, lme.1.8, time_var = "followupyears") 

jointFit.9<-jm(baseline.cox.9, lme.1.9, time_var = "followupyears") 

#summary(jointFit.9)
```

```{r RubinÂ´s Rules}

# Create a list of the fitted joint models
joint_models_list <- list(jointFit.1, jointFit.2, jointFit.3, jointFit.4, 
                          jointFit.5, jointFit.6, jointFit.7, jointFit.8, jointFit.9)


#summary(jointFit.1)

#coefficients 

coef(jointFit.1) #this is a list

jointFit.1[["statistics"]][["Mean"]][["gammas"]] #This is numeric

#SD

jointFit.1[["statistics"]][["SD"]][["gammas"]] #This is numeric

#Standard error

jointFit.1[["statistics"]][["SE"]][["gammas"]] #This is numeric


#-------create list to store coef SD SE-----

# Initialize lists to store coefficients, SD, and SE
coefficients_list <- list()
sd_list <- list()
se_list <- list()

# Loop through the 9 models
for (i in 1:9) {
  # Extract coefficients
  coefficients <- joint_models_list[[i]][["statistics"]][["Mean"]][["gammas"]]
  
  # Extract SD
  sd <- joint_models_list[[i]][["statistics"]][["SD"]][["gammas"]]
  
  # Extract SE
  se <- joint_models_list[[i]][["statistics"]][["SE"]][["gammas"]]
  
  # Append to respective lists
  coefficients_list[[i]] <- coefficients
  sd_list[[i]] <- sd
  se_list[[i]] <- se
}


#-----Pooled coefficients------

# Initialize lists to store pooled coefficients 
pooled_coefficients <- numeric(length(coefficients_list[[1]]))

# Loop through each predictor and apply Rubin's rules
for (j in seq_along(coefficients_list[[1]])) {
  # Calculate pooled estimate for each parameter
  pooled_coefficients[j] <- mean(sapply(coefficients_list, function(coeff) coeff[j]))
}

# Pooled coefficients
pooled_coefficients


#-----Calculate the within-imputation variance (W)-----


# Calculate the number of predictors
num_predictors <- length(coefficients_list[[1]])

# Initialize a vector to store the within-imputation variances (W) for each predictor
within_imputation_variances <- numeric(num_predictors)

# Loop through each predictor (parameter)
for (j in 1:num_predictors) {
  # Calculate the within-imputation variance (W) for the current predictor
  within_imputation_variances[j] <- sum(sapply(sd_list, function(se) se[j]^2)) / 9
}


#------Calculate the between-imputation variance (B)------


coefficients_list[[1]][1]


# Initialize a vector to store the between-imputation variances (B) for each predictor
between_imputation_variances <- numeric(length(coefficients_list[[1]]))

# Loop through each predictor
for (j in seq_along(coefficients_list[[1]])) {
  # Initialize a variable to store the sum of squared differences for the current predictor
  sum_squared_diff <- 0
  
  # Calculate the sum of squared differences for the current predictor across all imputed datasets
  for (i in 1:9) {
    sum_squared_diff <- sum_squared_diff + (coefficients_list[[i]][j] - pooled_coefficients[j])^2
  }
  
  # Calculate the between-imputation variance (B) for the current predictor
  between_imputation_variances[j] <- sum_squared_diff / (9 - 1)
}


#------Calculate the total variance----

# Initialize a vector to store the total variances (T) for each predictor
total_variances <- numeric(length(coefficients_list[[1]]))

# Loop through each predictor
for (j in seq_along(coefficients_list[[1]])) {
  # Calculate the total variance (T) for the current predictor
  total_variances[j] <- within_imputation_variances[j]+(1+1/9)*between_imputation_variances[j] 
  #The imputation times is 9
}

# Calculate the standard deviations (SD) for each predictor from the total variances
pooled_sd <- sqrt(total_variances)

```


```{r Summary the pool estimated}

pooled_coefficients

pooled_sd

```


```{r need to figure out how to pool the estimate of mmse}


```


```{r }

# Loop through each study to extract and plot baseline hazard
for (study_id in study_names) {
  study_data <- subset(data.baseline.1, study == study_id)
  cox_model <- coxph(Surv(years, cens) ~ age10 + sex + mdsupdrs3.10 + hybl + hallucinationsindex + cognitiveindex, data = study_data, x = TRUE, model = TRUE)
  baseline_hazard <- basehaz(cox_model)
  baseline_hazard$hazard <- diff(c(0, baseline_hazard$hazard)) / diff(c(0, baseline_hazard$time))
  
  # Plot the baseline hazard for the current study
  plot(baseline_hazard$time, baseline_hazard$hazard, type = "s", xlab = "Time", ylab = "Hazard", main = paste("Baseline Hazard - Study", study_id))
}



```

```{r Note for next step}

# Need to find a Linear mixed model to fit MMSE

#Re run the imputation, to see if the cogntive index is yes and no will that be better?

#Need to think about how to do the rubin's rule.

```